#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 17 17:32:32 2017

@author: haotianteng
"""
import tensorflow as tf
from distutils.dir_util import copy_tree
from chiron_input import read_raw_data_sets
from cnn import getcnnfeature
from cnn import getcnnlogit
#from rnn import rnn_layers
from rnn import rnn_layers_one_direction
import time,os

def save_model():
    copy_tree(os.path.dirname(os.path.abspath(__file__)),FLAGS.log_dir+FLAGS.model_name+'/model')
def inference(x,seq_length,training):
    cnn_feature = getcnnfeature(x,training = training)
    feashape = cnn_feature.get_shape().as_list()
    ratio = FLAGS.sequence_len/feashape[1]
    logits = getcnnlogit(cnn_feature)
    return logits,ratio

def loss(logits,seq_len,label):
    loss = tf.reduce_mean(tf.nn.ctc_loss(label,logits,seq_len,ctc_merge_repeated = True,time_major = False))
    tf.summary.scalar('loss',loss)
    return loss

def train_step(loss,global_step = None):
    opt = tf.train.AdamOptimizer(FLAGS.step_rate).minimize(loss,global_step=global_step)
    return opt

def train():
    training = tf.placeholder(tf.bool)
    global_step=tf.get_variable('global_step',trainable=False,shape=(),dtype = tf.int32,initializer = tf.zeros_initializer())
    x = tf.placeholder(tf.float32,shape = [FLAGS.batch_size,FLAGS.sequence_len])
    seq_length = tf.placeholder(tf.int32, shape = [FLAGS.batch_size])
    y_indexs = tf.placeholder(tf.int64)
    y_values = tf.placeholder(tf.int32)
    y_shape = tf.placeholder(tf.int64)
    y = tf.SparseTensor(y_indexs,y_values,y_shape)
    logits,ratio = inference(x,seq_length,training)
    ctc_loss = loss(logits,seq_length,y)
    opt = train_step(ctc_loss,global_step = global_step)
    init = tf.global_variables_initializer()
    saver = tf.train.Saver()
    
    sess = tf.Session(config = tf.ConfigProto(allow_soft_placement=True))
    save_model()
    if FLAGS.retrain==False:
        sess.run(init)
        print("Model init finished, begin loading data. \n")
    
    train_ds = read_raw_data_sets(FLAGS.data_dir,FLAGS.cache_dir,FLAGS.sequence_len,k_mer = FLAGS.k_mer)
    start=time.time()
    for i in range(FLAGS.max_steps):
        batch_x,seq_len,batch_y = train_ds.next_batch(FLAGS.batch_size)
        indxs,values,shape = batch_y
        feed_dict =  {x:batch_x,seq_length:seq_len/ratio,y_indexs:indxs,y_values:values,y_shape:shape,training:True}
        loss_val,_ = sess.run([ctc_loss,opt],feed_dict = feed_dict)
    global_step_val = tf.train.global_step(sess,global_step)
    print "Model %s saved."%(FLAGS.log_dir+FLAGS.model_name)
    print "Reads number %d"%(train_ds.reads_n)       
    saver.save(sess,FLAGS.log_dir+FLAGS.model_name+'/final.ckpt',global_step=global_step_val)
def run(args):
    global FLAGS
    FLAGS=args
    FLAGS.data_dir = FLAGS.data_dir + os.path.sep
    FLAGS.log_dir = FLAGS.log_dir + os.path.sep
    train()

if __name__ == "__main__":
    class Flags():
     def __init__(self):
        self.data_dir = '/uufs/chpc.utah.edu/common/home/u1142888/Chiron-0.3/new_out'
        self.cache_dir = '/uufs/chpc.utah.edu/common/home/u1142888/Chiron-0.3/output_cache'
        self.log_dir = '/uufs/chpc.utah.edu/common/home/u1142888/Chiron-0.3'
        self.sequence_len = 300
        self.batch_size = 400
        self.step_rate = 1e-3 
        self.max_steps = 100
        self.k_mer = 1
        self.model_name = 'res50'
        self.retrain =False
    flags=Flags()
    run(flags)
        
        
